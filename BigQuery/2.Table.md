# 개요
1. BigQuery Tables
2. Table Limitation
3. Creating table
4. Managing Table
5. Managing Table's data

# BigQuery Tables
`BigQuery table`은 row들에 각각의 정보를 저장한다. 각 레코드는 column(as called as field)로 구성되어 있다.

모든 테이블은 컬럼명, 데이터 타입 등 정보를 나타내는 `schema`로 정의된다.

Big Query는 다음 3가지 타입의 테이블을 지원한다.

1. **Native Tables**: 기본 BigQuery저장소에서 지원되는 테이블.
2. **External tables**: BigQuery 외부 저장소에서 지원되는 테이블
3. **View**: SQL Query로 지정된 가상 테이블.


# Tablie Limitation
BigQuery 테이블은 다음의 제약사항을 가진다.

* 데이터세트마다 **테이블 이름이 고유**해야 합니다.
* BigQuery 웹 UI에서는 테이블을 한 번에 하나만 복사할 수 있습니다.
* **테이블을 복사할 때는 대상 데이터세트가 복사 중인 테이블과 동일한 위치**에 있어야 합니다. 예를 들어 EU 기반 데이터세트의 테이블을 US 기반 데이터세트에 복사할 수 없습니다.
* CLI나 API를 이용해 **여러 원본 테이블을 단일 대상 테이블로 복사할 때는 모든 원본 테이블의 스키마가 같**아야 합니다.
* BigQuery 웹 UI, 명령줄 도구, API를 사용하여** 테이블을 한 번에 하나만 삭제**할 수 있습니다.
* 테이블 데이터를 내보낼 때는 Google Cloud Storage만 대상으로 지원됩니다.
* 데이터세트의 테이블이 50,000개 이상이 되면 이를 열거하는 속도가 느려집니다. API 호출, BigQuery 웹 UI 또는 `__TABLES_SUMMARY__` 메타 테이블을 사용하는지 여부에 관계없이 열거 성능이 저하됩니다. UI 성능을 향상시키기 위해서는 `?minimal` 매개변수를 사용하여 로드 작업을 프로젝트당 30,000개 테이블로 제한할 수 있습니다. BigQuery 웹 UI URL에 매개변수를 https://bigquery.cloud.google.com/queries/[PROJECT_NAME]?minimal 형식으로 추가합니다.

# Creating table

### 스키마 정의가 있는 빈 테이블 만들기

`mk` 명령어와 `--table` 또는 `-t` 플래그를 사용합니다.  
테이블 스키마 정보는 `인라인`으로 제공하거나 `JSON 스키마 파일`로 제공할 수 있습니다.  
선택적 매개변수로는 `--expiration, --description, --time_partitioning_type, --destination_kms_key, --label`이 있습니다.  
기본 프로젝트가 아닌 다른 프로젝트에서 테이블을 생성하는 경우 프로젝트 ID를 `[PROJECT_ID]:[DATASET]` 형식으로 데이터세트에 추가합니다.

> `bq mk --table --expiration [INTEGER] --description [DESCRIPTION] --label [KEY:VALUE, KEY:VALUE] [PROJECT_ID]:[DATASET].[TABLE] [SCHEMA]`

### 쿼리 결과에서 테이블 만들기

`bq query `명령어를 입력하고 `--destination_table` 플래그를 지정해 쿼리 결과에 기반한 영구 테이블을 만듭니다.   
표준 SQL 구문을 사용하려면 `use_legacy_sql=false` 플래그를 지정하세요.  
기본 프로젝트에 없는 테이블에 쿼리 결과를 쓰려면 프로젝트 ID를 `[PROJECT_ID]:[DATASET]` 형식으로 데이터세트 이름에 추가합니다.

> `bq --location=[LOCATION] query --destination_table [PROJECT_ID]:[DATASET].[TABLE] --use_legacy_sql=false '[QUERY]'


### 데이터 로드 시 테이블 만들기

데이터를 로드하기 전에, 빈 테이블을 만들지 않아도 된다.  
테이블이 있다고 가정하고, 테이블 명과 매개변수를 적당히 입력하면 해당 테이블이 생성된다.

# Managing Table

테이블을 관리하는 방법
1. 테이블의 속성 업데이트
   1. 만료시간
   2. 설명
   3. 스키마 정의
   4. 라벨
2. 테이블 이름 바꾸기(복사)
3. 테이블 복사
4. 테이블 삭제
5. 삭제된 테이블 복원

### 1. 테이블 속성 업데이트
1. 만료시간
> `bq update --expiration [INTEGER] [PROJECT_ID]:[DATASET].[TABLE]`
2. 설명 (description)
> `bq update --description "[DESCRIPTION]" [PROJECT_ID]:[DATASET].[TABLE]`
3. 스키마 정의 - 
[Dataset & Schema](./1.Dataset%20%26%20Schema.md) 참조
4. 라벨
> `bq update --set_label [KEY:VALUE] [PROJECT_ID]:[DATASET].[TABLE_OR_VIEW]`

위 속성을 업데이트 할 수 있다.

### 2. 테이블 이름바꾸기 (복사)
현재는 기존 테이블의 이름을 변경할 수 없다. 테이블으 ㅣ이름을 변경해야 한다면 테이블 복사 단계를 따르고, 복사 잡업에서 대상 테이블을 지정할 때 새 테이블 이름을 사용하면 된다.

### 3. 테이블 복사
`BigQuery 웹 UI` 또는 `명령줄 도구`의 
> `bq cp` 명령어
> 
를 사용하거나, `jobs.insert API 메소드`를 호출하고 복사 작업을 구성하여 테이블을 복사할 수 있습니다.

##### Using CLI
`bq cp` 명령어를 실행합니다. 선택적 플래그를 사용하면 대상 테이블의 쓰기 처리를 제어할 수 있습니다.

* `-a` 또는 `--append_table`은 소스 테이블의 데이터를 대상 데이터세트의 기존 테이블에 추가합니다.  
* `-f` 또는 `--force`는 대상 데이터세트의 기존 테이블을 덮어쓰며 확인 메시지를 표시하지 않습니다.  
* `-n `또는 `--no_clobber`는 테이블이 대상 데이터세트에 있으면 다음과 같은 오류 메시지를 반환합니다.   Table '[PROJECT_ID]:[DATASET].[TABLE]' already exists, skipping. -n을 지정하지 않으면 기본 동작으로 대상 테이블을 교체할지 묻는 메시지를 표시합니다.  
* `--destination_kms_key`는 대상 테이블을 암호화하는 데 사용하는 고객 관리 Cloud KMS 키입니다.
* `--destination_kms_key`는 여기서 설명하지 않습니다. 자세한 내용은 Cloud KMS 키로 데이터 보호를 참조하세요.

* 소스 또는 대상 데이터세트가 기본 프로젝트가 아닌 다른 프로젝트에 있다면 프로젝트 ID를 `[PROJECT_ID]:[DATASET]` 형식으로 데이터세트 이름에 추가합니다.

* `--location` 플래그를 입력하고 값을 사용자 위치로 설정합니다.
> `bq --location=[LOCATION] cp -a -f -n [PROJECT_ID]:[DATASET].[SOURCE_TABLE] [PROJECT_ID]:[DATASET].[DESTINATION_TABLE]`

* `mydataset.mytable`과 `mydataset.mytable2`를 `mydataset2.tablecopy`로 복사하려면 다음 명령어를 입력합니다. 모든 데이터세트는 기본 프로젝트에 있으며 US 다중 지역 위치에서 생성되었습니다.

> `bq --location=US cp mydataset.mytable,mydataset.mytable2 mydataset2.tablecopy`

# Managing Table's data

BigQuery 테이블 데이터에서 다음의 작업을 수행할 수 있다.

* 테이블에 데이터 로드
* 테이블에 데이터 추가 또는 테이블 데이터 덮어쓰기
* 테이블 데이터 찾아보기(또는 미리보기)
* 테이블 데이터 쿼리
* 데이터 조작 언어를 사용하여 테이블 데이터 수정
* 테이블 데이터 복사
* 테이블 데이터 내보내기

## 테이블에 데이터 로드

Using CLI

`bq loa`d 명령어를 사용하고` source_format`을 지정하고 로컬 파일 경로를 포함시킵니다.   
`--location` 플래그를 제공하고 값을 사용자 위치로 설정합니다.

> `bq --location=[LOCATION] load --source_format=[FORMAT] [DATASET].[TABLE] [PATH_TO_SOURCE] [SCHEMA]`

| 웹 UI 옵션 | CLI 플래그 | description |
| --- | --- | --- |
| write if empty | `none` | 테이블이 비었으면 데이터 쓰기(CLI 에서는 기본.) |
| append to table | `--no-replace` or `--replace=false` | 기존 데이터 마지막에 데이터 추가. (CLI에서 기본)|
| overwrite table | `--replace` or `--replace=true` | 기존 데이터 삭제하고 데이터 추가 |


## 데이터 추가 또는 덮어쓰기

소스 파일과 쿼리 결과에서 추가 데이터를 테이블에 로드할 수 있습니다.  
데이터의 스키마가 대상 테이블 또는 파티션의 스키마와 일치하지 않는다면 추가하거나 덮어쓸 때 스키마를 업데이트하면 됩니다.

데이터를 추가할 때 스키마를 업데이트하면 
* 새로운 필드 추가
* `REQUIRED`필드를 `NULLABLE`로 완화  
할 수 있습니다.

## 테이블 데이터 내보내기

데이터를 BigQuery에 로드한 후, 여러가지 형식으로 데이터를 export할 수 있다.  
최대 1GB의 데이터를 하나의 파일로 내보낼 수 있고, 1GB가 넘는 데이터는 여러파일로 내보내야 한다.

### 내보내기 형식 및 압축 유형

| 데이터 | 형식	지원되는 압축 유형 |	세부정보 |
| --- | --- | --- |
| `CSV` |	`GZIP`	 | `--field_delimiter` CLI 플래그 또는 `configuration.extract.fieldDelimiter` 추출 작업 속성을 사용하여 내보낸 데이터의 `CSV` 구분 기호를 제어할 수 있습니다. 중첩 및 반복 데이터가 지원되지 않습니다. |
| `JSON`	| `GZIP`	| 중첩 및 반복 데이터가 지원됩니다. |
| `Avro` |	`DEFLATE`, `SNAPPY`	| `Avro` 내보내기에는 `GZIP` 압축이 지원되지 않습니다. 중첩 및 반복 데이터가 지원됩니다.|

### 테이블 데이터 내보내기

> `bq --location=[LOCATION] extract --destination_format [FORMAT] --compression [COMPRESSION_TYPE] --field_delimiter [DELIMITER] --print_header [BOOLEAN] [PROJECT_ID]:[DATASET].[TABLE] gs://[BUCKET]/[FILENAME]`

* `[FORMAT]`은 내보낸 데이터 형식(`CSV`, `NEWLINE_DELIMITED_JSON` 또는 `AVRO`)입니다.
* `[COMPRESSION_TYPE]`은 데이터 형식에 지원되는 압축 유형입니다. `CSV` 및 `NEWLINE_DELIMITED_JSON은`, `GZIP`을 지원합니다. `AVRO는` `DEFLATE` 및 `SNAPPY를` 지원합니다.
* `[DELIMITER]`는 `CSV` 내보내기의 열 사이의 경계를 나타내는 문자입니다. `\t` 및 `tab`은 탭에 허용되는 이름입니다.
* `[BOOLEAN]`은 `true` 또는 `false입니다`. `true`로 설정하면 헤더 행은 데이터 형식이 헤더를 지원하는 경우 내보낸 데이터에 인쇄됩니다. 기본값은 true입니다.
* `[FILENAME]`은 내보낸 데이터 파일의 이름입니다. [와일드 카드](https://cloud.google.com/bigquery/docs/exporting-data#exporting_data_into_one_or_more_files)를 사용하여 여러 파일로 내보낼 수 있습니다.
